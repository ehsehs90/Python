{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAN20lEQVR4nO3db6hcdX7H8c/HdOOfGELSXGNwY7NKHlSLzYZBjcpika5/nuiKW9eAKCxGRGEXN1BNAys+kFCqi2BZzFbZKFZZ1FQR2aphMeaBS8YYNRrbqKSbmJjcRGHVPLCJ3z64J+Ua75y5mXNmzuR+3y+4zMz5zjnny0k+98yd35n5OSIEYOo7oekGAAwGYQeSIOxAEoQdSIKwA0n8xSB3Nnfu3Fi4cOEgdwmksmPHDu3fv98T1SqF3fYVkh6UNE3Sv0XE6rLnL1y4UO12u8ouAZRotVodaz2/jLc9TdK/SrpS0jmSbrB9Tq/bA9BfVf5mP1/SBxHxUUR8JekpSVfX0xaAulUJ+xmSdo57vKtY9g22l9tu226Pjo5W2B2AKqqEfaI3Ab517W1ErImIVkS0RkZGKuwOQBVVwr5L0oJxj78raXe1dgD0S5Wwb5K0yPb3bE+X9BNJz9fTFoC69Tz0FhGHbN8h6T81NvT2aES8W1tnAGpVaZw9Il6U9GJNvQDoIy6XBZIg7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRRacpm2zskfS7psKRDEdGqoykA9asU9sLfRcT+GrYDoI94GQ8kUTXsIekl22/YXj7RE2wvt9223R4dHa24OwC9qhr2iyNiiaQrJd1u+wdHPyEi1kREKyJaIyMjFXcHoFeVwh4Ru4vbfZLWSTq/jqYA1K/nsNueYXvmkfuSfihpa12NAahXlXfj50laZ/vIdv49In5fS1cAatdz2CPiI0l/W2MvAPqIoTcgCcIOJEHYgSQIO5AEYQeSqOODMGjYK6+80rFWDI12NHv27NL61q3ll04sXbq0tL5o0aLSOgaHMzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJDFlxtk3bNhQWn/99ddL6/fff3+d7QzUgQMHel532rRppfWvvvqqtH7KKaeU1k899dSOtUsuuaR03ccff7zSvvFNnNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IInjapx99erVHWurVq0qXffw4cN1tzMlVD0uBw8e7Ln+7LPPlq7b7bP4a9euLa3PmDGjtJ4NZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSOK4Gmd/+OGHO9a6jRdfeOGFpfWZM2f21FMdLrvsstL6tddeO6BOjt1LL71UWn/wwQc71rZv31667jPPPNNTT0c89thjHWsZPwvf9cxu+1Hb+2xvHbdsju2XbW8vbstnGgDQuMm8jP+tpCuOWnaXpPURsUjS+uIxgCHWNewRsUHSp0ctvlrSkWsV10q6pua+ANSs1zfo5kXEHkkqbk/r9ETby223bbdHR0d73B2Aqvr+bnxErImIVkS0RkZG+r07AB30Gva9tudLUnG7r76WAPRDr2F/XtJNxf2bJD1XTzsA+sURUf4E+0lJl0qaK2mvpF9K+g9Jv5N0pqQ/SfpxRBz9Jt63tFqtaLfbPTe7f//+jrUPP/ywdN3FixeX1k888cSeekK5zz77rGOt2/UFb775ZqV9P/HEEx1ry5Ytq7TtYdVqtdRutyf8IoCuF9VExA0dSuX/UgCGCpfLAkkQdiAJwg4kQdiBJAg7kETXobc6VR16w9TSbRrtpUuXVtr+vHnzOtY++eSTStseVmVDb5zZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IInjaspmHH+ee67zlAIbN27s676//PLLjrWdO3eWrrtgwYK622kcZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9ingiy++6Fhbt25d6bqrVq2qu51vKBvP7vecBWXH5bzzzitdt2yq6eNV1zO77Udt77O9ddyye2x/bHtL8XNVf9sEUNVkXsb/VtIVEyz/VUQsLn5erLctAHXrGvaI2CDp0wH0AqCPqrxBd4ftt4uX+bM7Pcn2cttt2+3R0dEKuwNQRa9h/7WksyUtlrRH0v2dnhgRayKiFRGtkZGRHncHoKqewh4ReyPicER8Lek3ks6vty0Adesp7Lbnj3v4I0lbOz0XwHDoOs5u+0lJl0qaa3uXpF9KutT2YkkhaYekW/vY45T33nvvldY3bdpUWl+9enXH2vvvv99TT1PdihUrmm5h4LqGPSJumGDxI33oBUAfcbkskARhB5Ig7EAShB1IgrADSfAR1xocOHCgtH7bbbeV1p9++unSej8/Cnr22WeX1k8//fRK23/ooYc61qZPn1667rJly0rrb731Vk89SdKZZ57Z87rHK87sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+yT9NRTT3Ws3XvvvaXrbtu2rbQ+c+bM0vqcOXNK6/fdd1/HWreph7t9pfKsWbNK6/1U9ZuNynq//PLLK237eMSZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9kl599dWOtW7j6DfffHNpfeXKlaX1RYsWldaPVx9//HFpvdtXbHdz0kkndayddtpplbZ9POLMDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+SQ888EDH2pIlS0rXveWWW+puZ0rYuXNnaX337t2Vtn/ddddVWn+q6Xpmt73A9h9sb7P9ru2fFcvn2H7Z9vbidnb/2wXQq8m8jD8k6RcR8deSLpR0u+1zJN0laX1ELJK0vngMYEh1DXtE7ImIzcX9zyVtk3SGpKslrS2etlbSNf1qEkB1x/QGne2Fkr4v6Y+S5kXEHmnsF4KkCS82tr3cdtt2e3R0tFq3AHo26bDbPlXSM5J+HhF/nux6EbEmIloR0ar6BYIAejepsNv+jsaC/kREPFss3mt7flGfL2lff1oEUIeuQ2+2LekRSdsiYvz40/OSbpK0urh9ri8dDomTTz65Y42htd6UfWx4Mrp9xfadd95ZaftTzWTG2S+WdKOkd2xvKZat1FjIf2f7p5L+JOnH/WkRQB26hj0iNkpyh/Jl9bYDoF+4XBZIgrADSRB2IAnCDiRB2IEk+Igr+uqCCy7oWNu8eXOlbV9//fWl9bPOOqvS9qcazuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7OirsumsDx06VLru7NnlX1i8YsWKnnrKijM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBODsqee2110rrBw8e7FibNWtW6bovvPBCaZ3Pqx8bzuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMRk5mdfIOkxSadL+lrSmoh40PY9km6RNFo8dWVEvNivRtGMw4cPl9bvvvvu0vr06dM71rrNa3/RRReV1nFsJnNRzSFJv4iIzbZnSnrD9stF7VcR8S/9aw9AXSYzP/seSXuK+5/b3ibpjH43BqBex/Q3u+2Fkr4v6Y/Fojtsv237UdsTfoeQ7eW227bbo6OjEz0FwABMOuy2T5X0jKSfR8SfJf1a0tmSFmvszH//ROtFxJqIaEVEa2RkpIaWAfRiUmG3/R2NBf2JiHhWkiJib0QcjoivJf1G0vn9axNAVV3DbtuSHpG0LSIeGLd8/rin/UjS1vrbA1CXybwbf7GkGyW9Y3tLsWylpBtsL5YUknZIurUvHaJRY7/rO7v11vJ/9iVLlnSsnXvuuT31hN5M5t34jZIm+hdnTB04jnAFHZAEYQeSIOxAEoQdSIKwA0kQdiAJvkoapU44ofx8cOONNw6oE1TFmR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHknBEDG5n9qik/xm3aK6k/QNr4NgMa2/D2pdEb72qs7e/iogJv/9toGH/1s7tdkS0GmugxLD2Nqx9SfTWq0H1xst4IAnCDiTRdNjXNLz/MsPa27D2JdFbrwbSW6N/swMYnKbP7AAGhLADSTQSdttX2P4v2x/YvquJHjqxvcP2O7a32G433MujtvfZ3jpu2RzbL9veXtxOOMdeQ73dY/vj4thtsX1VQ70tsP0H29tsv2v7Z8XyRo9dSV8DOW4D/5vd9jRJ/y3p7yXtkrRJ0g0R8d5AG+nA9g5JrYho/AIM2z+Q9IWkxyLib4pl/yzp04hYXfyinB0R/zgkvd0j6Yump/EuZiuaP36acUnXSLpZDR67kr7+QQM4bk2c2c+X9EFEfBQRX0l6StLVDfQx9CJig6RPj1p8taS1xf21GvvPMnAdehsKEbEnIjYX9z+XdGSa8UaPXUlfA9FE2M+QtHPc410arvneQ9JLtt+wvbzpZiYwLyL2SGP/eSSd1nA/R+s6jfcgHTXN+NAcu16mP6+qibBPNJXUMI3/XRwRSyRdKen24uUqJmdS03gPygTTjA+FXqc/r6qJsO+StGDc4+9K2t1AHxOKiN3F7T5J6zR8U1HvPTKDbnG7r+F+/t8wTeM90TTjGoJj1+T0502EfZOkRba/Z3u6pJ9Ier6BPr7F9ozijRPZniHphxq+qaifl3RTcf8mSc812Ms3DMs03p2mGVfDx67x6c8jYuA/kq7S2DvyH0r6pyZ66NDXWZLeKn7ebbo3SU9q7GXd/2rsFdFPJf2lpPWSthe3c4aot8clvSPpbY0Fa35DvV2isT8N35a0pfi5quljV9LXQI4bl8sCSXAFHZAEYQeSIOxAEoQdSIKwA0kQdiAJwg4k8X8XPil57gqOOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "31.459873\n",
      "6.7823987\n",
      "5.294943\n",
      "1.353095\n",
      "0.0\n",
      "0.0\n",
      "1.1785486\n",
      "0.4781871\n",
      "0.0\n",
      "0.636471\n",
      "정확도 : 0.9574000239372253\n"
     ]
    }
   ],
   "source": [
    "##기본 MNIST 예제(multinomial classification)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np  \n",
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "\n",
    "#Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)  \n",
    "##Data set 을 불러들이는 코드작성. (input data란 객체 제공해줌. read_data_set이란 함수 제공해줌)\n",
    "#one_hot encoding처리까지 자동적으로 해줌\n",
    "\n",
    "##데이터 확인\n",
    "print(mnist.train.num_examples) #학습용 데이터의 개수\n",
    "print(mnist.train.images.shape) #(55000,784)\n",
    "#우리가 불러들인 mnist에서 학습용 데이터(image/label) 중 image data볼거에요. (가로 28,세로 28 픽셀정보가 담긴 Data)\n",
    "# 28X28이미지를 1차원 형태로 저장\n",
    "\n",
    "print(mnist.train.labels.shape)\n",
    "\n",
    "plt.imshow(mnist.train.images[0].reshape(28,28),\n",
    "           cmap=\"Greys\", interpolation=\"nearest\")  ##55000개중 첫번째 이미지에 대한 픽셀정보. (이미지 를 x,y축에 맞춰 그려야하니까 1차원을 2차원으로 reshape해주는거임)\n",
    "plt.show()\n",
    "print(mnist.train.labels[0])\n",
    "\n",
    "\n",
    "#placeholder\n",
    "X=tf.placeholder(shape=[None,784], dtype=tf.float32)  ##행부분은 나중에 예측할때 문제가 생기므로 55000이아닌None, 열부분만 표기\n",
    "Y=tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "#Weight & bias\n",
    "W1=tf.Variable(tf.random_normal([784,256]), name=\"weight1\")  \n",
    "#10 여기 숫자는 정해져있는게 없음 2번째 layeer에 내가 몇개의 input을 쓸거냐 \n",
    "#에따라 달라짐.숫자가 클수록 더 많이 학습한다는 뜻임.\n",
    "b1=tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "layer1=tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([256,256]), name=\"weight2\")  #2 2  2output과 input과의 갯수만 맞추자\n",
    "b2=tf.Variable(tf.random_normal([256]), name=\"bias2\") #너의 output은 y와 맞\n",
    "layer2=tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([256,10]), name=\"weight3\")  #2 2  2output과 input과의 갯수만 맞추자\n",
    "b3=tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "#layer3=tf.sigmoid(tf.matmul(X,W1)+b3)\n",
    "\n",
    "# hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "\n",
    "#cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "\n",
    "#train node생성\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "#Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    \n",
    "training_epoch = 30 #(30번 반복한다는 뜻)\n",
    "batch_size = 100 # 55000개의 행을 다 읽어들이는게 아니라 100 개의 행을 읽어서 반복 학습.(100개씩 잘라서 55000개를 읽을거에요-2중루프 필요. 에폭포르푸, 배치포루프)\n",
    "\n",
    "for step in range(training_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size )\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y =mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train,cost],\n",
    "                              feed_dict={X:batch_x,Y:batch_y})\n",
    "    if step % 3 ==0:\n",
    "        print(cost_val)\n",
    "\n",
    "#Accuracy 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy =tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "result=sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})\n",
    "print(\"정확도 : {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sigmoid는 값이 점점 희미해지는 경향을 띈다.\n",
    "##이거 말고 더 좋은 형태\n",
    "##랠루~가즈앗 = relu\n",
    "# GradientDescent\n",
    "# 이;거말고 더 좋은거\n",
    "\n",
    "# AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 61\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 싸이키런\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Data Loading\n",
    "mnist = pd.read_csv('./data/kaggle/train.csv')\n",
    "mnist_test = pd.read_csv('./data/kaggle/test.csv')\n",
    "\n",
    "df_x = mnist.drop('label', axis=1, inplace=False)\n",
    "df_y = mnist['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_data = MinMaxScaler().fit_transform(tf.one_hot(df_y, depth=10).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:1656.6617431640625\n",
      "cost:0.31314945220947266\n",
      "cost:1.221781440108316e-05\n",
      "cost:4.575949333229801e-06\n",
      "cost:2.7049982236349024e-06\n",
      "cost:1.8528792224969948e-06\n",
      "cost:1.3658564057550393e-06\n",
      "cost:1.052695779435453e-06\n",
      "cost:8.360435117538145e-07\n",
      "cost:6.778888632652524e-07\n"
     ]
    }
   ],
   "source": [
    "# placeholder \n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([256]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "# 두번째  Layer .layer1의 output 개수가 layer2의 input 개수와 같아야 한다.\n",
    "W2 = tf.Variable(tf.random_normal([256,256]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "\n",
    "\n",
    "# hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "# Session & 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "\n",
    "num_of_epoch =30\n",
    "batch_size=100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    total_iter=int(mnist.train.num_examles/batch_size)\n",
    "    for i in range(total_iter):\n",
    "    ([train, cost], feed_dict = {X:x_data, Y:y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(\"cost:{}\".format(cost_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================\n",
    "# train.csv\n",
    "# => 7:3 비율로 나누어서 학습 및 accuracy 확인\n",
    "# -batch 형태로 데이터를 읽어들여서 학습\n",
    "# test.csv를 이용하여 prediction 결과도출\n",
    "\n",
    "# 해당 결과를 이용하여\n",
    "# submission.csv파일을 생헝한 후 Kanggle site에 제출 및 확인.\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#07_16\n",
    "#왜 Relu가 더 좋나?\n",
    "#sigmoid 를 사용하면 layer가 갈수록(?) 흐려지는 경향성을 띈다.\n",
    "#\n",
    "#초기 Weight값 초기화 문제\n",
    "#초기값을 0으로 주면 안됨. ->그러면 어떻게하면 초기값을 잘 조성할 수 있을까!\n",
    "#Weight의 초기화를 위해 RBM이라고 불리는 복잡한 방법을 이용.\n",
    "#또는 수식보다 라이브러리가 제공해주는 걸 사용하는게 효율이 훨씬 좋음.\n",
    "#W1=tf.Variable(tf.random_normal([784,256]), name=\"weight\")\n",
    "#\n",
    "#Neural Network Overfitting을 방지하기 위한 간단한 방법\n",
    "#Dropout : random하게 몇 개의 neuron을 zero로 setting\n",
    "#\n",
    "#layer1=tf.nn.dropout(_layer1, keep_prob=keep_prob)  :::::::::keep_prob : 몇 %의 자료를 사용할것인가.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7969481\n",
      "0.036978927\n",
      "Accuracy: 0.980571448802948\n"
     ]
    }
   ],
   "source": [
    "# 기본 MNIST(multinomial classification)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Loading\n",
    "train_data = pd.read_csv(\"./data/kaggle/train.csv\")\n",
    "train_x_data = train_data.drop('label', axis = 1)\n",
    "train_y_data = tf.one_hot(train_data[\"label\"], depth=10).eval(session = tf.Session())\n",
    "test_x_data = pd.read_csv(\"./data/kaggle/test.csv\")\n",
    "\n",
    "# Tensorflow Graph Initialization\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W1 = tf.get_variable(\"weight1\", shape = [784,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape = [256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name = \"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape = [256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = \"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y))\n",
    "\n",
    "# train node\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# session object & initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch & batch size\n",
    "training_epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "# training\n",
    "for step in range(training_epoch):\n",
    "    num_of_iteration = int(train_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = train_x_data[i*batch_size:(i+1)*batch_size],train_y_data[i*batch_size:(i+1)*batch_size]\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "\n",
    "    if step %5 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#predict check\n",
    "predict = tf.argmax(H,1)\n",
    "result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "df = pd.DataFrame({\n",
    "    'ImageId': [i for i in range(1,28001)],\n",
    "    'Label': result\n",
    "})\n",
    "df.to_csv('./data/kaggle/submission.csv', index=False)\n",
    "\n",
    "correct = tf.equal(predict, tf.math.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict = {X: train_x_data, Y: train_y_data, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980571448802948\n"
     ]
    }
   ],
   "source": [
    "#predict check\n",
    "predict = tf.argmax(H,1)\n",
    "result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "df = pd.DataFrame({\n",
    "    'ImageId': [i for i in range(1,28001)],\n",
    "    'Label': result\n",
    "})\n",
    "df.to_csv('./data/kaggle/submission.csv', index=False)\n",
    "\n",
    "correct = tf.equal(predict, tf.math.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict = {X: train_x_data, Y: train_y_data, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me the Money\n"
     ]
    }
   ],
   "source": [
    "a=\"Show me the Code\"\n",
    "result=a.replace(\"Code\",\"Money\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Show', 'me', 'the', 'Code']\n"
     ]
    }
   ],
   "source": [
    "a=\"Show me the Code\"\n",
    "result=a.split()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '', 'b', '', 'c', '', 'd', '', 'e']\n"
     ]
    }
   ],
   "source": [
    "a=\"a::b::c::d::e\"\n",
    "result=a.split(\":\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
      "1Hello\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "print(a*3)\n",
    "print(str(a[0])+\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "range(1, 11, 2)\n",
      "range(1, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "range_2=range(1,11,2)\n",
    "print(range_2.index(7))\n",
    "print(range_2)\n",
    "print(range_2[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': '홍길동', 'age': 30, 10: 'kk', 'hobby': 'game'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a={\"name\":\"홍길동\",\"age\":30}\n",
    "print(type(a))                        \n",
    "a[10]=\"kk\"                              \n",
    "a[\"hobby\"]=\"game\"                      \n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "서울\n"
     ]
    }
   ],
   "source": [
    "dict_1={\"name\":\"홍길동\",\"age\":30,\"age\":40}\n",
    "dict_2={\"name\":\"홍길동\",\"age\":30,(\"address:,\"):\"서울\"}\n",
    "print(dict_2[\"age\"])\n",
    "print(dict_2[(\"address:,\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
