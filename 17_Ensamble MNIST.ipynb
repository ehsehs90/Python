{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([[0., 1., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    299\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 300\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    301\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3477\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3478\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3566\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[1;32m-> 3567\u001b[1;33m                                                            types_str))\n\u001b[0m\u001b[0;32m   3568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-799929bb04e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;31m#train_y_data.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1137\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \"\"\"\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    302\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[0;32m    303\u001b[0m                         \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m                         (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[0;32m    305\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument array([[0., 1., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "1# 기본 MNIST(multinomial classification)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Loading\n",
    "train_data = pd.read_csv(\"./data/kaggle/train.csv\")\n",
    "num_of_train = int(mnist.shape[0]*0.7)\n",
    "train_data = mnist.loc[:num_of_train,:]\n",
    "test_data = mnist.loc[num_of_train+1:,:]\n",
    "\n",
    "\n",
    "train_x_data = train_data.drop('label', axis = 1, inplace=False)\n",
    "train_y_data = tf.one_hot(train_data[\"label\"], depth=10).eval(session = tf.Session())\n",
    "test_x_data = pd.read_csv(\"./data/kaggle/test.csv\")\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(train_y_data)\n",
    "#train_y_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tensorflow Graph Initialization\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(shape = [None, 784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None, 10], dtype = tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "X_img = tf.reshape(X, [-1,28,28,1])\n",
    "\n",
    "#필터 만들고 convolution 하고 relu작업까지 한방에 가능~(실제구현)\n",
    "L1 = tf.layers.conv2d(inputs=X_img, filters=32,\n",
    "                     kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1,activation=tf.nn.relu)\n",
    "L1 = tf.layers.max_pooling2d(inputs=L1,\n",
    "                            pool_size=[2,2], \n",
    "                            padding=\"SAME\",\n",
    "                            strides=2)\n",
    "\n",
    "#필터 만들고 convolution 하고 relu작업까지 한방에 가능~(실제구현)  << 2 >>\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64,\n",
    "                     kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1,activation=tf.nn.relu)\n",
    "L2 = tf.layers.max_pooling2d(inputs=L2,\n",
    "                            pool_size=[2,2], \n",
    "                            padding=\"SAME\",\n",
    "                            strides=2)\n",
    "\n",
    "print(L2.shape)\n",
    "\n",
    "L2 = tf.reshape(L2,[-1,7*7*64])\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", shape=[7*7*64,256],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W1)+b1)\n",
    "layer1= tf.layers.dropout(_layer1, rate=drop_rate:0.3)\n",
    "\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape=[256,10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([10]), name=\"bias2\")\n",
    "\n",
    "\n",
    "# # Hypothesis\n",
    "# logits = tf.matmul(layer2,W3) + b3\n",
    "# H = tf.nn.relu(logits)\n",
    "\n",
    "# # cost function\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y))\n",
    "\n",
    "# # train node\n",
    "# train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# # session object & initialization\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# # epoch & batch size\n",
    "# training_epoch = 10\n",
    "# batch_size = 100\n",
    "\n",
    "# # training\n",
    "# for step in range(training_epoch):\n",
    "#     num_of_iteration = int(train_data.shape[0] / batch_size)\n",
    "#     cost_val = 0\n",
    "    \n",
    "#     for i in range(num_of_iteration):\n",
    "#         batch_x, batch_y = train_x_data[i*batch_size:(i+1)*batch_size],train_y_data[i*batch_size:(i+1)*batch_size]\n",
    "#         _, cost_val = sess.run([train, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "\n",
    "#     if step %5 == 0:\n",
    "#         print(cost_val)\n",
    "        \n",
    "# #predict check\n",
    "# predict = tf.argmax(H,1)\n",
    "# result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "# df = pd.DataFrame({\n",
    "#     'ImageId': [i for i in range(1,28001)],\n",
    "#     'Label': result\n",
    "# })\n",
    "# df.to_csv('./data/kaggle/submission.csv', index=False)\n",
    "\n",
    "# correct = tf.equal(predict, tf.math.argmax(Y,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "# print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict = {X: train_x_data, Y: train_y_data, keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-50455d88a6fc>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-50455d88a6fc>:46: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-50455d88a6fc>:50: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "(?, 7, 7, 64)\n",
      "WARNING:tensorflow:From <ipython-input-1-50455d88a6fc>:72: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "Epoch: 0001 cost = 0.181926642\n",
      "Epoch: 0002 cost = 0.049479124\n",
      "Epoch: 0003 cost = 0.035585340\n",
      "Epoch: 0004 cost = 0.024622918\n",
      "Epoch: 0005 cost = 0.018872435\n",
      "Epoch: 0006 cost = 0.012684673\n",
      "Epoch: 0007 cost = 0.011407548\n",
      "Epoch: 0008 cost = 0.010107332\n",
      "Epoch: 0009 cost = 0.007390559\n",
      "Epoch: 0010 cost = 0.006556607\n",
      "Epoch: 0011 cost = 0.006961521\n",
      "Epoch: 0012 cost = 0.007529905\n",
      "Epoch: 0013 cost = 0.001637278\n",
      "Epoch: 0014 cost = 0.004658490\n",
      "Epoch: 0015 cost = 0.005085441\n"
     ]
    }
   ],
   "source": [
    "#필요한 모듈 import \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 1. Data Loading\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "\n",
    "# 2. Model정의(Tensorflow graph 생성)\n",
    "\n",
    "tf.reset_default_graph() #tensorflow graph초기화\n",
    "# 2.1 placeholder\n",
    "\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "drop_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "#2.2 Convolution\n",
    "## cNN은 이미지 학습에 최적화된 deep learning방법\n",
    "##입력받은 이미지의 형태가 4차원 배열\n",
    "##(이미지의 개수, 이미지의 width, 이미지의 height, color수)\n",
    "X_img = tf.reshape(X, [-1,28,28,1])\n",
    "\n",
    "# ## 2.3 Convolution Layer 1\n",
    "# ## filter 정의 => \n",
    "# filter의 shape(width, height, color, filter수)\n",
    "# filter1 = tf.Variable(tf.random_normal([3,3,1,32]))\n",
    "\n",
    "# #filter를 이용해서 Convolution image를 생성\n",
    "# L1 = tf.nn.conv2d(X_img, filter1, strides=[1,1,1,1], padding = \"SAME\")\n",
    "\n",
    "# #만들어진 Convolution에 Relu를 적용\n",
    "# L1 = tf.nn.relu(L1)\n",
    " \n",
    "# ##pooling 작업(resize, sampling 작업) => optional\n",
    "# #풀링작업하는이유 -> 필터적용해서 하다보면 이미지값이 계속 커짐.\n",
    "\n",
    "# L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "#필터 만들고 convolution 하고 relu작업까지 한방에 가능~(실제구현)\n",
    "L1 = tf.layers.conv2d(inputs=X_img, filters=32,\n",
    "                     kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1,activation=tf.nn.relu)\n",
    "L1 = tf.layers.max_pooling2d(inputs=L1,\n",
    "                            pool_size=[2,2], \n",
    "                            padding=\"SAME\",\n",
    "                            strides=2)\n",
    "\n",
    "#필터 만들고 convolution 하고 relu작업까지 한방에 가능~(실제구현)  << 2 >>\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64,\n",
    "                     kernel_size=[3,3], padding=\"SAME\",\n",
    "                     strides=1,activation=tf.nn.relu)\n",
    "L2 = tf.layers.max_pooling2d(inputs=L2,\n",
    "                            pool_size=[2,2], \n",
    "                            padding=\"SAME\",\n",
    "                            strides=2)\n",
    "\n",
    "print(L2.shape)\n",
    "#(?, 7, 7, 64)\n",
    "#2.3 Neural Network\n",
    "#Convolution 의 결과를 Neural Network 의 입력으로 사용하기 위해 shape을 변경\n",
    "\n",
    "L2 = tf.reshape(L2,[-1,7*7*64])\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", shape=[7*7*64,256],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W1)+b1)\n",
    "layer1= tf.layers.dropout(_layer1, rate=drop_rate)\n",
    "\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape=[256,10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([10]), name=\"bias2\")\n",
    "\n",
    "\n",
    "#hypothesis\n",
    "logits = tf.matmul(layer1,W2)+b2\n",
    "H= tf.nn.relu(logits)\n",
    "\n",
    "\n",
    "#cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "##train\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train=optimizer.minimize(cost)\n",
    "# #\n",
    "# #Session. 초기화\n",
    "# sess=tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# #학습진행(batch처리)\n",
    "\n",
    "#Session. 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# parameters\n",
    "\n",
    "training_epochs = 15\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "# train my model\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0\n",
    "\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, drop_rate: 0.7}\n",
    "\n",
    "        c, _ = sess.run([cost, train], feed_dict=feed_dict)\n",
    "\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "\n",
    "\n",
    "#Accuracy 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
